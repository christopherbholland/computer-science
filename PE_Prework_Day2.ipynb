{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christopherbholland/computer-science/blob/master/PE_Prework_Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEsWTSCg5zcQ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def prompt(language_model, system_prompt, task_definition):\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": system_prompt},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": task_definition\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "import datetime\n",
        "\n",
        "def save_to_file(language_model, system_prompt, task_definition, results):\n",
        "  filename = datetime.datetime.now()\n",
        "  filename = filename.strftime(\"%Y%m%d%H%M%S%f\")\n",
        "  file = open(\"./\" + filename + \".txt\", 'w')\n",
        "  file_contents = f\"\"\"\n",
        "  Language Model: {language_model}\n",
        "  System Prompt: {system_prompt}\n",
        "  Task Definition: {task_definition}\n",
        "  Results: {results}\n",
        "  \"\"\"\n",
        "  file.write(file_contents)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"gpt-4o-mini\"]\n",
        "\n",
        "system_prompts = [\"You are a poem-writing agent. Please write a nice poem.\"]\n",
        "\n",
        "task_definitions = [\"Please write a poem about snakes.\",\n",
        "                   \"Please write a haiku about snakes.\",\n",
        "                   \"Please write a haiku about using snakes as a metaphor for life.\",\n",
        "                   \"Please write a haiku about using snakes as a metaphor for death.\"]\n",
        "\n",
        "# For each model we're testing\n",
        "for language_model in models:\n",
        "    # For each system prompt\n",
        "    for system_prompt in system_prompts:\n",
        "        # For each task definition\n",
        "        for task_definition in task_definitions:\n",
        "            # Query the model and get results\n",
        "            results = prompt(language_model, system_prompt, task_definition)\n",
        "            # Save results to file\n",
        "            save_to_file(language_model, system_prompt, task_definition, results)"
      ],
      "metadata": {
        "id": "PudksbhoRIzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T62n9tiIKIlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I want to test which system prompts produce the best outputs, given some task definitions\n",
        "\n",
        "## I want to test multiple language models\n",
        "models = [\"gpt-4o-mini\"]\n",
        "\n",
        "## I want to have a list of system prompts\n",
        "system_prompts = [\"You are a poem-writing agent. Please write a nice poem.\"]\n",
        "\n",
        "## I want to have a list of task definitions:\n",
        "task_definitions = [\n",
        "  \"Please write a poem about snakes\",\"Please write a haiku about snakes.\",\n",
        "  \"Please write a haiku about using snakes as a metaphor for life\",\n",
        "  \"Please write a haiku about using snakes as a metaphor for death\"\n",
        "]\n",
        "\n",
        "## for each model we're testing\n",
        "for language_model in models:\n",
        "  ## For each system prompt:\n",
        "  for system_prompt in system_prompts:\n",
        "    ## Run a function that:\n",
        "    ## give a system prompt\n",
        "    for task in task_definitions:\n",
        "      ## for each task definition\n",
        "    ## given a series of task definitions that I want to run\n",
        "        ## query the specified language model with each task definition with the system prompt separately as a new conversation\n",
        "      results = prompt(language_model, system_prompt, task_definition)\n",
        "      ## get the results, and write them to a file\n",
        "      save_to_file(language_model, system_prompt, task_definition)\n",
        "\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def prompt(language_model, system_prompt, task_definition):\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": task_definition\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "94kAsBzo-1Bd"
      }
    }
  ]
}